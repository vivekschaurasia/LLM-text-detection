# LLM-text-detection


This project processes and analyzes text data using machine learning techniques, with a focus on distinguishing between text generated by humans and text generated by large language models (LLMs). It employs Word2Vec for word embeddings, autoencoders for dimensionality reduction, and additional evaluation methods to classify and analyze text data effectively.

## Features

- **Data Preprocessing**: Cleans and organizes datasets for training and testing.
- **Word2Vec Embedding**: Converts text data into vector representations.
- **Autoencoders**: Encodes and reconstructs text data using neural networks for dimensionality reduction.
- **BLEU Score Evaluation**: Compares reconstructed and original text for quality assessment.
- **Visualization**: Generates visual insights into data distribution and prediction performance.
  
## Project Structure
 
1. **Data Loading and Cleaning**:
   - Load multiple datasets.
   - Preprocess text data by cleaning and feature extraction.

2. **Word2Vec Embedding**:
   - Train Word2Vec models for different classes of text data.
   - Save and reload embeddings for downstream tasks.

3. **Autoencoder Training**:
   - Build and train autoencoders for different classes.
   - Use reconstruction loss for classification.

4. **Prediction and Evaluation**:
   - Evaluate model predictions using BLEU scores and accuracy metrics.
   - Compare results for different classes of text.

5. **Visualization**:
   - Visualize data distributions, word embeddings, and reconstruction results.

## Dependencies

This project uses the following Python libraries:

- **Data Manipulation**:
  - `numpy`
  - `pandas`
- **Visualization**:
  - `seaborn`
  - `matplotlib`
- **Natural Language Processing**:
  - `gensim` (for Word2Vec)
  - `nltk` (for BLEU score evaluation)
- **Machine Learning**:
  - `tensorflow` (for autoencoders)
  - `sklearn` (for metrics like accuracy and confusion matrix)

## How to Use

1. **Install Dependencies**:
   Install the required libraries using pip:
   ```bash
   pip install numpy pandas seaborn matplotlib gensim tensorflow scikit-learn nltk
   ```

2. **Run the Notebook**:
   Execute the notebook to preprocess the data, train models, and evaluate results.

3. **Customize Parameters**:
   Modify dataset paths and model parameters (e.g., embedding dimensions, training epochs) as needed.

## Results

The project demonstrates a workflow for:
- Analyzing text data.
- Building and evaluating machine learning models for text classification.
- Visualizing key insights from the text data.

## Future Work

- Optimize autoencoder architectures for better reconstruction quality.
- Explore additional metrics for model evaluation.
- Experiment with larger datasets and advanced embedding techniques like Transformers.
